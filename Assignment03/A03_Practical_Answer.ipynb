{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmagonmNEpn0"
      },
      "source": [
        "## Important instruction\n",
        "\n",
        "For programming exercises only edit the code as shown in the following format.\n",
        "\n",
        "```\n",
        "##############################################\n",
        "\n",
        "#Edit the following code\n",
        "\n",
        "var1 = 3\n",
        "var2 = 4\n",
        "print(var1 + var4)\n",
        "\n",
        "##############################################\n",
        "```\n",
        "\n",
        "You are open to experimenting with the other parts of code but you will only be awarded points if the question asked is answered which only needs finishing or making changes to the code in the above specified format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Eilr2YrEsMF"
      },
      "source": [
        "## Question 7: Maximum Likelihood Estimation for Gaussian Distribution\n",
        "\n",
        "You are given a dataset ```data_points``` that follows a Gaussian distribution. Your task is to find mean, variance and log likelihood by completing the functions ```calculate_mean_variance``` and ```gaussian_log_likelihood```.\n",
        "\n",
        "*For more information, refer to **4.2.3 Gaussian (Normal) Density** section of **Introduction to Machine Learning, 4th Edition, The MIT Press, 2020** by Ethem Alpaydın.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1wD6HAvZyUJi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean: 3.4599999999999995\n",
            "Variance: 2.2543999999999995\n",
            "Log Likelihood: -9.12690232142906\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_mean_variance(data_points):\n",
        "    n = len(data_points)\n",
        "    ############################################################\n",
        "    #Edit the following code\n",
        "\n",
        "    mean = np.sum(data_points) / n\n",
        "    variance = np.var(data_points)  # or np.sum((data_points - mean) ** 2) / n\n",
        "    \n",
        "    ############################################################\n",
        "    return mean, variance\n",
        "\n",
        "def gaussian_log_likelihood(mean, variance, data_points):\n",
        "    n = len(data_points)\n",
        "    ############################################################\n",
        "    #Edit the following code\n",
        "\n",
        "    log_likelihood = -n / 2 * np.log(2 * np.pi * variance) - np.sum((data_points - mean) ** 2) / (2 * variance)\n",
        "    ############################################################\n",
        "    return log_likelihood\n",
        "\n",
        "data_points = np.array([1.2, 2.5, 3.8, 4.2, 5.6])\n",
        "\n",
        "# Calculating mean and variance\n",
        "mean, variance = calculate_mean_variance(data_points)\n",
        "\n",
        "# Calculating log likelihood\n",
        "log_likelihood = gaussian_log_likelihood(mean, variance, data_points)\n",
        "\n",
        "print(\"Mean:\", mean)\n",
        "print(\"Variance:\", variance)\n",
        "print(\"Log Likelihood:\", log_likelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo9L3FCBnr6g"
      },
      "source": [
        "## Question 8: Analyzing Bias, Variance, and Error in Regression Models\n",
        "\n",
        "Consider a dataset of oceanographic temperature and salinity measurements. The dataset has been loaded and split into training and testing sets. Three regression models have been applied to predict salinity based on temperature: Linear Regression, Polynomial Regression of degree 2, and Polynomial Regression of degree 5. The `calculate_metrics` function has been defined to compute bias, squared bias, variance, and error for each model.\n",
        "\n",
        "Analyze the performance metrics (bias, squared bias, variance, and error) for each regression model obtained by finishing `calculate_metrics` funtion. Compare and contrast the models in terms of their bias, variance, and overall predictive performance.\n",
        "\n",
        "\n",
        "*For more information, refer to **4.7  Tuning Model Complexity: Bias/Variance Dilemma** section of **Introduction to Machine Learning, 4th Edition, The MIT Press, 2020** by Ethem Alpaydın. and [numpy documentation](https://numpy.org/doc/stable/reference/routines.statistics.html) for various attributes.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qn3DzVBPw4za"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression:\n",
            "Bias: -0.003, Variance: 0.201, Error: 0.064\n",
            "\n",
            "Polynomial Regression (Degree 2):\n",
            "Bias: -0.007, Variance: 0.205, Error: 0.061\n",
            "\n",
            "Polynomial Regression (Degree 5):\n",
            "Bias: -0.001, Variance: 0.225, Error: 0.036\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def calculate_metrics(model, X_test, y_test):\n",
        "    ############################################################\n",
        "    #Edit the following code\n",
        "    y_pred = model.predict(X_test)\n",
        "    true_mean = np.mean(y_test)\n",
        "    bias = np.mean(y_pred) - true_mean\n",
        "    bias_squared = bias ** 2\n",
        "    variance = np.var(y_pred)\n",
        "    error = np.mean((y_test - y_pred) ** 2)\n",
        "    #do not use mean_squared_error from scikit learn package\n",
        "    ############################################################\n",
        "    return bias, bias_squared, variance, error\n",
        "\n",
        "# Load the oceanographic dataset from the provided link\n",
        "url = \"https://raw.githubusercontent.com/JakeMWu/single-linear-regression-CalCOFI-oceanographic-data/main/tempsal.csv\"\n",
        "oceanographic_data = pd.read_csv(url, nrows=800)  # Use only the first 300 rows\n",
        "\n",
        "# Remove rows with NaN values in 'T_degC' or 'Salnty'\n",
        "oceanographic_data = oceanographic_data.dropna(subset=['T_degC', 'Salnty'])\n",
        "\n",
        "# Use 'T_degC' as the feature and 'Salnty' as the target variable\n",
        "X = oceanographic_data['T_degC'].values.reshape(-1, 1)\n",
        "y = oceanographic_data['Salnty'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Polynomial Regression (Degree 2)\n",
        "poly_model = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
        "poly_model.fit(X_train, y_train)\n",
        "\n",
        "# Polynomial Regression (Degree 5)\n",
        "poly_model5 = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
        "poly_model5.fit(X_train, y_train)\n",
        "\n",
        "# Calculate metrics for each model\n",
        "bias_linear, _ ,variance_linear, error_linear = calculate_metrics(linear_model, X_test, y_test)\n",
        "bias_poly, _ ,variance_poly, error_poly = calculate_metrics(poly_model, X_test, y_test)\n",
        "bias_poly5, _, variance_poly5, error_poly5 = calculate_metrics(poly_model5, X_test, y_test)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"Bias: {bias_linear:.3f}, Variance: {variance_linear:.3f}, Error: {error_linear:.3f}\\n\")\n",
        "\n",
        "print(\"Polynomial Regression (Degree 2):\")\n",
        "print(f\"Bias: {bias_poly:.3f}, Variance: {variance_poly:.3f}, Error: {error_poly:.3f}\\n\")\n",
        "\n",
        "print(\"Polynomial Regression (Degree 5):\")\n",
        "print(f\"Bias: {bias_poly5:.3f}, Variance: {variance_poly5:.3f}, Error: {error_poly5:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqSVkC5ZpDVJ"
      },
      "source": [
        "### **Answer the following question with a brief reasoning.**\n",
        "Based on the results of bias, variance and error, explain which regression model is best suited for the above data and why"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nJyEBgtpXTD"
      },
      "source": [
        "The Polynomial Regression (Degree 5) model appears to be the best suited for the data, as it has the lowest error (0.036) among the three models. Despite its slightly higher variance (0.225) compared to the Linear Regression (0.201) and Polynomial Regression (Degree 2) (0.205), the significant reduction in error indicates that the Degree 5 model captures the underlying pattern of the data more accurately without overly compromising due to increased variance. The very small biases across all models (-0.003 for Linear, -0.007 for Degree 2, and -0.001 for Degree 5) suggest that all models are, on average, very close to the actual values, but the substantial decrease in error with the Degree 5 model highlights its superior predictive performance. This suggests that the complexity added by the Degree 5 polynomial features is justified by the considerable gain in modeling the dataset's nuances, making it the preferable choice despite the inherent risk of higher variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-YHaYgb4Taj"
      },
      "source": [
        "## Question 9: AIC and BIC Evaluation in Regression Analysis - Model Performance\n",
        "\n",
        "A regression analysis is performed in the following code using three different models: Linear Regression, Lasso Regression, and Ridge Regression. Calculate the AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) for each model by completing the ```calculate_aic_bic``` function.\n",
        "\n",
        "*For more information, refer to **4.8  Tuning Model Complexity: Bias/Variance Dilemma** section of **Introduction to Machine Learning, 4th Edition, The MIT Press, 2020** by Ethem Alpaydın and Lecture 05: Parametric Method slides from class.*\n",
        "\n",
        "*Further reading on https://vitalflux.com/aic-vs-bic-for-regression-models-formula-examples/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O2bNDLi_Podv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+---------+---------+----------+\n",
            "| Model             |     AIC |     BIC |      MSE |\n",
            "+===================+=========+=========+==========+\n",
            "| Linear Regression | 68.2704 | 74.2448 | 0.975938 |\n",
            "+-------------------+---------+---------+----------+\n",
            "| Lasso Regression  | 67.9644 | 73.9388 | 0.96112  |\n",
            "+-------------------+---------+---------+----------+\n",
            "| Ridge Regression  | 68.2765 | 74.2509 | 0.976235 |\n",
            "+-------------------+---------+---------+----------+\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tabulate import tabulate\n",
        "\n",
        "def calculate_aic_bic(y_true, y_pred, num_params, n):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    log_likelihood = -0.5 * len(y_true) * np.log(2 * np.pi * mse) - 0.5 * len(y_true)\n",
        "    ############################################################\n",
        "    #Edit the following code\n",
        "    aic = 2 * num_params - 2 * log_likelihood\n",
        "    bic = np.log(n) * num_params - 2 * log_likelihood\n",
        "    ############################################################\n",
        "    return aic, bic, mse\n",
        "\n",
        "# Generating sample data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)\n",
        "y = 2*X[:, 0] + 3*X[:, 1] - 4*X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "linear_predictions = linear_model.predict(X_test)\n",
        "linear_params = X_train.shape[1] + 1  # Including the intercept\n",
        "linear_aic, linear_bic, linear_mse = calculate_aic_bic(y_test, linear_predictions, linear_params, len(y_test))\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=0.01)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "lasso_predictions = lasso_model.predict(X_test)\n",
        "lasso_params = np.count_nonzero(lasso_model.coef_) + 1  # Including the intercept\n",
        "lasso_aic, lasso_bic, lasso_mse = calculate_aic_bic(y_test, lasso_predictions, lasso_params, len(y_test))\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_model = Ridge(alpha=0.01)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_predictions = ridge_model.predict(X_test)\n",
        "ridge_params = X_train.shape[1] + 1  # Including the intercept\n",
        "ridge_aic, ridge_bic, ridge_mse = calculate_aic_bic(y_test, ridge_predictions, ridge_params, len(y_test))\n",
        "\n",
        "# Create a table to print the values\n",
        "table = [[\"Linear Regression\", linear_aic, linear_bic, linear_mse],\n",
        "         [\"Lasso Regression\", lasso_aic, lasso_bic, lasso_mse],\n",
        "         [\"Ridge Regression\", ridge_aic, ridge_bic, ridge_mse]]\n",
        "\n",
        "headers = [\"Model\", \"AIC\", \"BIC\", \"MSE\"]\n",
        "print(tabulate(table, headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvymVt6D5GoT"
      },
      "source": [
        "### **Answer the following question with a brief reasoning.**\n",
        "Determine the best model based on the AIC and BIC results. Provide reasoning for your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxa5VLCB5GoU"
      },
      "source": [
        "The Lasso Regression model is favored over Linear and Ridge Regression models based on lower AIC and BIC values, indicating optimal balance between fit and complexity. Its regularization technique, which reduces overfitting by eliminating less important features, makes it the best choice for generalizability and simplicity, outperforming others in terms of model efficiency and effectiveness for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vCRarDTHxyu"
      },
      "source": [
        "## Question 10: Linear regression with  k-fold cross validation\n",
        "\n",
        "Your task to implement a linear regression with using k-fold cross validation model using scikit learn package of python. Please follow the below instructions to complete the exercise.\n",
        "\n",
        "1. **Split the Data:**\n",
        "   - Use `train_test_split` to split the data into training, validation, and test sets with a test size of 20% and a random state of 99.\n",
        "\n",
        "2. **Set Up K-Fold Cross-Validation:**\n",
        "   - Set the number of folds for cross-validation to 5.\n",
        "   - Initialize a KFold cross-validator with shuffling and a random state of 99.\n",
        "\n",
        "3. **Initialize Linear Regression Model:**\n",
        "   - Create an instance of the linear regression model.\n",
        "\n",
        "4. **Perform K-Fold Cross-Validation:**\n",
        "   - Use a loop to iterate through the folds generated by KFold.\n",
        "   - For each fold, train the model on the training data, make predictions on both training and validation sets.\n",
        "   - For each fold, calculate mean squared errors.\n",
        "   - Append the errors to lists for further analysis.\n",
        "\n",
        "5. **Fit the Model on the Entire Training + Validation Set:**\n",
        "   - After cross-validation, fit the model on the entire training + validation set to utilize all available data for training.\n",
        "\n",
        "6. **Make Predictions on the Test Set:**\n",
        "   - Use the trained model to make predictions on the test set.\n",
        "\n",
        "\n",
        "For more information, refer https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html and https://www.kaggle.com/code/jnikhilsai/cross-validation-with-linear-regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "75reHCuO51KJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Training Error: 0.00390418409025593\n",
            "Average Validation Error: 0.004228318833330217\n",
            "Test Error: 0.004303691767806357\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment%202/question12/linear_data.csv')\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df.drop('Chance of Admit', axis=1)\n",
        "y = df['Chance of Admit']\n",
        "\n",
        "# Lists to store performance metrics for each fold\n",
        "train_errors, val_errors = [], []\n",
        "\n",
        "############################################################\n",
        "#Write the code for the corresponding instructions mentioned\n",
        "\n",
        "# Split the data into training, validation, and test sets based on the provided instructions\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size = 0.2, random_state = 99)\n",
        "\n",
        "# Set the number of folds for cross-validation\n",
        "k_folds = 5\n",
        "\n",
        "# Initialize KFold cross-validator\n",
        "kf = KFold(n_splits=k_folds, shuffle = True, random_state= 99)\n",
        "\n",
        "# Initialize linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "for train_index, val_index in kf.split(X_train_val):\n",
        "    X_train, X_val = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n",
        "    y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[val_index]\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the training and validation sets\n",
        "    y_train_pred = model.predict(X_train) \n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate mean squared error for training and validation sets\n",
        "    train_error = mean_squared_error(y_train, y_train_pred)\n",
        "    val_error = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "    # Append errors to the lists\n",
        "    train_errors.append(train_error)\n",
        "    val_errors.append(val_error)\n",
        "\n",
        "# Fit the model on the entire training + validation set\n",
        "model.fit(X_train_val, y_train_val)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "############################################################\n",
        "\n",
        "# Calculate mean squared error for the test set\n",
        "test_error = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Calculate the average training and validation errors\n",
        "avg_train_error = np.mean(train_errors)\n",
        "avg_val_error = np.mean(val_errors)\n",
        "\n",
        "print(f'Average Training Error: {avg_train_error}')\n",
        "print(f'Average Validation Error: {avg_val_error}')\n",
        "print(f'Test Error: {test_error}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
