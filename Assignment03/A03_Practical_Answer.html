<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>4363a2e8259144c1878fb806d5f9a411</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="important-instruction" class="cell markdown"
id="pmagonmNEpn0">
<h2>Important instruction</h2>
<p>For programming exercises only edit the code as shown in the
following format.</p>
<pre><code>##############################################

#Edit the following code

var1 = 3
var2 = 4
print(var1 + var4)

##############################################</code></pre>
<p>You are open to experimenting with the other parts of code but you
will only be awarded points if the question asked is answered which only
needs finishing or making changes to the code in the above specified
format.</p>
</section>
<section
id="question-7-maximum-likelihood-estimation-for-gaussian-distribution"
class="cell markdown" id="1Eilr2YrEsMF">
<h2>Question 7: Maximum Likelihood Estimation for Gaussian
Distribution</h2>
<p>You are given a dataset <code>data_points</code> that follows a
Gaussian distribution. Your task is to find mean, variance and log
likelihood by completing the functions
<code>calculate_mean_variance</code> and
<code>gaussian_log_likelihood</code>.</p>
<p><em>For more information, refer to <strong>4.2.3 Gaussian (Normal)
Density</strong> section of <strong>Introduction to Machine Learning,
4th Edition, The MIT Press, 2020</strong> by Ethem Alpaydın.</em></p>
</section>
<div class="cell code" data-execution_count="17" id="1wD6HAvZyUJi">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_mean_variance(data_points):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(data_points)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Edit the following code</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> np.<span class="bu">sum</span>(data_points) <span class="op">/</span> n</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> np.var(data_points)  <span class="co"># or np.sum((data_points - mean) ** 2) / n</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean, variance</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gaussian_log_likelihood(mean, variance, data_points):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(data_points)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Edit the following code</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> <span class="op">-</span>n <span class="op">/</span> <span class="dv">2</span> <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> variance) <span class="op">-</span> np.<span class="bu">sum</span>((data_points <span class="op">-</span> mean) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> variance)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_likelihood</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>data_points <span class="op">=</span> np.array([<span class="fl">1.2</span>, <span class="fl">2.5</span>, <span class="fl">3.8</span>, <span class="fl">4.2</span>, <span class="fl">5.6</span>])</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating mean and variance</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>mean, variance <span class="op">=</span> calculate_mean_variance(data_points)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculating log likelihood</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>log_likelihood <span class="op">=</span> gaussian_log_likelihood(mean, variance, data_points)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean:&quot;</span>, mean)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Variance:&quot;</span>, variance)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Log Likelihood:&quot;</span>, log_likelihood)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Mean: 3.4599999999999995
Variance: 2.2543999999999995
Log Likelihood: -9.12690232142906
</code></pre>
</div>
</div>
<section
id="question-8-analyzing-bias-variance-and-error-in-regression-models"
class="cell markdown" id="eo9L3FCBnr6g">
<h2>Question 8: Analyzing Bias, Variance, and Error in Regression
Models</h2>
<p>Consider a dataset of oceanographic temperature and salinity
measurements. The dataset has been loaded and split into training and
testing sets. Three regression models have been applied to predict
salinity based on temperature: Linear Regression, Polynomial Regression
of degree 2, and Polynomial Regression of degree 5. The
<code>calculate_metrics</code> function has been defined to compute
bias, squared bias, variance, and error for each model.</p>
<p>Analyze the performance metrics (bias, squared bias, variance, and
error) for each regression model obtained by finishing
<code>calculate_metrics</code> funtion. Compare and contrast the models
in terms of their bias, variance, and overall predictive
performance.</p>
<p><em>For more information, refer to <strong>4.7 Tuning Model
Complexity: Bias/Variance Dilemma</strong> section of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem Alpaydın. and <a
href="https://numpy.org/doc/stable/reference/routines.statistics.html">numpy
documentation</a> for various attributes.</em></p>
</section>
<div class="cell code" data-execution_count="20" id="Qn3DzVBPw4za">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_metrics(model, X_test, y_test):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Edit the following code</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    true_mean <span class="op">=</span> np.mean(y_test)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> np.mean(y_pred) <span class="op">-</span> true_mean</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    bias_squared <span class="op">=</span> bias <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> np.var(y_pred)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> np.mean((y_test <span class="op">-</span> y_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#do not use mean_squared_error from scikit learn package</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bias, bias_squared, variance, error</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the oceanographic dataset from the provided link</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">&quot;https://raw.githubusercontent.com/JakeMWu/single-linear-regression-CalCOFI-oceanographic-data/main/tempsal.csv&quot;</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>oceanographic_data <span class="op">=</span> pd.read_csv(url, nrows<span class="op">=</span><span class="dv">800</span>)  <span class="co"># Use only the first 300 rows</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with NaN values in &#39;T_degC&#39; or &#39;Salnty&#39;</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>oceanographic_data <span class="op">=</span> oceanographic_data.dropna(subset<span class="op">=</span>[<span class="st">&#39;T_degC&#39;</span>, <span class="st">&#39;Salnty&#39;</span>])</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Use &#39;T_degC&#39; as the feature and &#39;Salnty&#39; as the target variable</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> oceanographic_data[<span class="st">&#39;T_degC&#39;</span>].values.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> oceanographic_data[<span class="st">&#39;Salnty&#39;</span>].values</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Regression</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>linear_model.fit(X_train, y_train)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial Regression (Degree 2)</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>poly_model <span class="op">=</span> make_pipeline(PolynomialFeatures(<span class="dv">2</span>), LinearRegression())</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>poly_model.fit(X_train, y_train)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial Regression (Degree 5)</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>poly_model5 <span class="op">=</span> make_pipeline(PolynomialFeatures(<span class="dv">5</span>), LinearRegression())</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>poly_model5.fit(X_train, y_train)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics for each model</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>bias_linear, _ ,variance_linear, error_linear <span class="op">=</span> calculate_metrics(linear_model, X_test, y_test)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>bias_poly, _ ,variance_poly, error_poly <span class="op">=</span> calculate_metrics(poly_model, X_test, y_test)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>bias_poly5, _, variance_poly5, error_poly5 <span class="op">=</span> calculate_metrics(poly_model5, X_test, y_test)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Print performance metrics</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Linear Regression:&quot;</span>)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Bias: </span><span class="sc">{</span>bias_linear<span class="sc">:.3f}</span><span class="ss">, Variance: </span><span class="sc">{</span>variance_linear<span class="sc">:.3f}</span><span class="ss">, Error: </span><span class="sc">{</span>error_linear<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Polynomial Regression (Degree 2):&quot;</span>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Bias: </span><span class="sc">{</span>bias_poly<span class="sc">:.3f}</span><span class="ss">, Variance: </span><span class="sc">{</span>variance_poly<span class="sc">:.3f}</span><span class="ss">, Error: </span><span class="sc">{</span>error_poly<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Polynomial Regression (Degree 5):&quot;</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Bias: </span><span class="sc">{</span>bias_poly5<span class="sc">:.3f}</span><span class="ss">, Variance: </span><span class="sc">{</span>variance_poly5<span class="sc">:.3f}</span><span class="ss">, Error: </span><span class="sc">{</span>error_poly5<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Linear Regression:
Bias: -0.003, Variance: 0.201, Error: 0.064

Polynomial Regression (Degree 2):
Bias: -0.007, Variance: 0.205, Error: 0.061

Polynomial Regression (Degree 5):
Bias: -0.001, Variance: 0.225, Error: 0.036

</code></pre>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="JqSVkC5ZpDVJ">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>Based on the results of bias, variance and error, explain which
regression model is best suited for the above data and why</p>
</section>
<div class="cell markdown" id="0nJyEBgtpXTD">
<p>The Polynomial Regression (Degree 5) model appears to be the best
suited for the data, as it has the lowest error (0.036) among the three
models. Despite its slightly higher variance (0.225) compared to the
Linear Regression (0.201) and Polynomial Regression (Degree 2) (0.205),
the significant reduction in error indicates that the Degree 5 model
captures the underlying pattern of the data more accurately without
overly compromising due to increased variance. The very small biases
across all models (-0.003 for Linear, -0.007 for Degree 2, and -0.001
for Degree 5) suggest that all models are, on average, very close to the
actual values, but the substantial decrease in error with the Degree 5
model highlights its superior predictive performance. This suggests that
the complexity added by the Degree 5 polynomial features is justified by
the considerable gain in modeling the dataset's nuances, making it the
preferable choice despite the inherent risk of higher variance.</p>
</div>
<section
id="question-9-aic-and-bic-evaluation-in-regression-analysis---model-performance"
class="cell markdown" id="w-YHaYgb4Taj">
<h2>Question 9: AIC and BIC Evaluation in Regression Analysis - Model
Performance</h2>
<p>A regression analysis is performed in the following code using three
different models: Linear Regression, Lasso Regression, and Ridge
Regression. Calculate the AIC (Akaike Information Criterion) and BIC
(Bayesian Information Criterion) for each model by completing the
<code>calculate_aic_bic</code> function.</p>
<p><em>For more information, refer to <strong>4.8 Tuning Model
Complexity: Bias/Variance Dilemma</strong> section of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem Alpaydın and Lecture 05: Parametric Method slides
from class.</em></p>
<p><em>Further reading on <a
href="https://vitalflux.com/aic-vs-bic-for-regression-models-formula-examples/"
class="uri">https://vitalflux.com/aic-vs-bic-for-regression-models-formula-examples/</a></em></p>
</section>
<div class="cell code" data-execution_count="1" id="O2bNDLi_Podv">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression, Lasso, Ridge</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_aic_bic(y_true, y_pred, num_params, n):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> mean_squared_error(y_true, y_pred)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="bu">len</span>(y_true) <span class="op">*</span> np.log(<span class="dv">2</span> <span class="op">*</span> np.pi <span class="op">*</span> mse) <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="bu">len</span>(y_true)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Edit the following code</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    aic <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> num_params <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> log_likelihood</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    bic <span class="op">=</span> np.log(n) <span class="op">*</span> num_params <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> log_likelihood</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">############################################################</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> aic, bic, mse</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating sample data</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">100</span>, <span class="dv">5</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>X[:, <span class="dv">0</span>] <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>X[:, <span class="dv">1</span>] <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>X[:, <span class="dv">2</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Regression</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>linear_model.fit(X_train, y_train)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>linear_predictions <span class="op">=</span> linear_model.predict(X_test)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>linear_params <span class="op">=</span> X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>  <span class="co"># Including the intercept</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>linear_aic, linear_bic, linear_mse <span class="op">=</span> calculate_aic_bic(y_test, linear_predictions, linear_params, <span class="bu">len</span>(y_test))</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Lasso Regression</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>lasso_model <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>lasso_model.fit(X_train, y_train)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>lasso_predictions <span class="op">=</span> lasso_model.predict(X_test)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>lasso_params <span class="op">=</span> np.count_nonzero(lasso_model.coef_) <span class="op">+</span> <span class="dv">1</span>  <span class="co"># Including the intercept</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>lasso_aic, lasso_bic, lasso_mse <span class="op">=</span> calculate_aic_bic(y_test, lasso_predictions, lasso_params, <span class="bu">len</span>(y_test))</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Ridge Regression</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>ridge_model <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>ridge_model.fit(X_train, y_train)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>ridge_predictions <span class="op">=</span> ridge_model.predict(X_test)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>ridge_params <span class="op">=</span> X_train.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>  <span class="co"># Including the intercept</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>ridge_aic, ridge_bic, ridge_mse <span class="op">=</span> calculate_aic_bic(y_test, ridge_predictions, ridge_params, <span class="bu">len</span>(y_test))</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a table to print the values</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> [[<span class="st">&quot;Linear Regression&quot;</span>, linear_aic, linear_bic, linear_mse],</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>         [<span class="st">&quot;Lasso Regression&quot;</span>, lasso_aic, lasso_bic, lasso_mse],</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>         [<span class="st">&quot;Ridge Regression&quot;</span>, ridge_aic, ridge_bic, ridge_mse]]</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> [<span class="st">&quot;Model&quot;</span>, <span class="st">&quot;AIC&quot;</span>, <span class="st">&quot;BIC&quot;</span>, <span class="st">&quot;MSE&quot;</span>]</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tabulate(table, headers, tablefmt<span class="op">=</span><span class="st">&quot;grid&quot;</span>))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>+-------------------+---------+---------+----------+
| Model             |     AIC |     BIC |      MSE |
+===================+=========+=========+==========+
| Linear Regression | 68.2704 | 74.2448 | 0.975938 |
+-------------------+---------+---------+----------+
| Lasso Regression  | 67.9644 | 73.9388 | 0.96112  |
+-------------------+---------+---------+----------+
| Ridge Regression  | 68.2765 | 74.2509 | 0.976235 |
+-------------------+---------+---------+----------+
</code></pre>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="bvymVt6D5GoT">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>Determine the best model based on the AIC and BIC results. Provide
reasoning for your choice.</p>
</section>
<div class="cell markdown" id="Nxa5VLCB5GoU">
<p>The Lasso Regression model is favored over Linear and Ridge
Regression models based on lower AIC and BIC values, indicating optimal
balance between fit and complexity. Its regularization technique, which
reduces overfitting by eliminating less important features, makes it the
best choice for generalizability and simplicity, outperforming others in
terms of model efficiency and effectiveness for this dataset.</p>
</div>
<section id="question-10-linear-regression-with-k-fold-cross-validation"
class="cell markdown" id="6vCRarDTHxyu">
<h2>Question 10: Linear regression with k-fold cross validation</h2>
<p>Your task to implement a linear regression with using k-fold cross
validation model using scikit learn package of python. Please follow the
below instructions to complete the exercise.</p>
<ol>
<li><strong>Split the Data:</strong>
<ul>
<li>Use <code>train_test_split</code> to split the data into training,
validation, and test sets with a test size of 20% and a random state of
99.</li>
</ul></li>
<li><strong>Set Up K-Fold Cross-Validation:</strong>
<ul>
<li>Set the number of folds for cross-validation to 5.</li>
<li>Initialize a KFold cross-validator with shuffling and a random state
of 99.</li>
</ul></li>
<li><strong>Initialize Linear Regression Model:</strong>
<ul>
<li>Create an instance of the linear regression model.</li>
</ul></li>
<li><strong>Perform K-Fold Cross-Validation:</strong>
<ul>
<li>Use a loop to iterate through the folds generated by KFold.</li>
<li>For each fold, train the model on the training data, make
predictions on both training and validation sets.</li>
<li>For each fold, calculate mean squared errors.</li>
<li>Append the errors to lists for further analysis.</li>
</ul></li>
<li><strong>Fit the Model on the Entire Training + Validation
Set:</strong>
<ul>
<li>After cross-validation, fit the model on the entire training +
validation set to utilize all available data for training.</li>
</ul></li>
<li><strong>Make Predictions on the Test Set:</strong>
<ul>
<li>Use the trained model to make predictions on the test set.</li>
</ul></li>
</ol>
<p>For more information, refer <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html</a>
and <a
href="https://www.kaggle.com/code/jnikhilsai/cross-validation-with-linear-regression"
class="uri">https://www.kaggle.com/code/jnikhilsai/cross-validation-with-linear-regression</a></p>
</section>
<div class="cell code" data-execution_count="25" id="75reHCuO51KJ">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, train_test_split</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment%202/question12/linear_data.csv&#39;</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features and target variable</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">&#39;Chance of Admit&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">&#39;Chance of Admit&#39;</span>]</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Lists to store performance metrics for each fold</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>train_errors, val_errors <span class="op">=</span> [], []</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">#Write the code for the corresponding instructions mentioned</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training, validation, and test sets based on the provided instructions</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>X_train_val, X_test, y_train_val, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state <span class="op">=</span> <span class="dv">99</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of folds for cross-validation</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>k_folds <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize KFold cross-validator</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>k_folds, shuffle <span class="op">=</span> <span class="va">True</span>, random_state<span class="op">=</span> <span class="dv">99</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize linear regression model</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform k-fold cross-validation</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train_index, val_index <span class="kw">in</span> kf.split(X_train_val):</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    X_train, X_val <span class="op">=</span> X_train_val.iloc[train_index], X_train_val.iloc[val_index]</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    y_train, y_val <span class="op">=</span> y_train_val.iloc[train_index], y_train_val.iloc[val_index]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fit the model on the training data</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make predictions on the training and validation sets</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    y_train_pred <span class="op">=</span> model.predict(X_train) </span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> model.predict(X_val)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean squared error for training and validation sets</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    train_error <span class="op">=</span> mean_squared_error(y_train, y_train_pred)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    val_error <span class="op">=</span> mean_squared_error(y_val, y_val_pred)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Append errors to the lists</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    train_errors.append(train_error)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    val_errors.append(val_error)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model on the entire training + validation set</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_val, y_train_val)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean squared error for the test set</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>test_error <span class="op">=</span> mean_squared_error(y_test, y_test_pred)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the average training and validation errors</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>avg_train_error <span class="op">=</span> np.mean(train_errors)</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>avg_val_error <span class="op">=</span> np.mean(val_errors)</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Average Training Error: </span><span class="sc">{</span>avg_train_error<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Average Validation Error: </span><span class="sc">{</span>avg_val_error<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Test Error: </span><span class="sc">{</span>test_error<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Average Training Error: 0.00390418409025593
Average Validation Error: 0.004228318833330217
Test Error: 0.004303691767806357
</code></pre>
</div>
</div>
</body>
</html>
