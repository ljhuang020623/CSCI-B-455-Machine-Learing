<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>f2b87230aa154e19b8692219a6442260</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="important-instruction" class="cell markdown"
id="E2WR-yMIpcbm">
<h2>Important instruction</h2>
<p>For programming exercises that only editing, only edit the code as
shown in the following format.</p>
<pre><code>##############################################

#Edit the following code

var1 = 3
var2 = 4
print(var1 + var4)

##############################################</code></pre>
<p>You are open to experimenting with the other parts of code but you
will only be awarded points if the question asked is answered which only
needs finishing or making changes to the code in the above specified
format.</p>
</section>
<section
id="question-7-dimensionality-reduction-using-principal-component-analysis-pca-and-t-distributed-stochastic-neighbor-embedding-t-sne"
class="cell markdown" id="H6jfkH9upe5T">
<h2>Question 7: Dimensionality reduction using Principal Component
Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding
(t-SNE).</h2>
<p>Your task to implement PCA and t-SNE on Wine dataset from
<code>sklearn.datasets</code>. Please follow the below instructions to
complete the exercise.</p>
<ol>
<li>Load the Wine dataset from <code>sklearn.datasets</code> and
standardize the features using <code>StandardScaler</code> from
<code>sklearn.preprocessing</code>.</li>
<li>The task is to perform PCA on the <strong>standardized data</strong>
and experiment with different values of the <code>n_components</code>
parameter. The goal is to find a value of <code>n_components</code> that
captures variance above 90% and less than 95% (Check out
<code>explained_variance_ratio_</code> attribute of PCA from sklearn
documentation mentioned in the references).</li>
<li>Implement t-SNE on <strong>standardized data</strong> where
<code>n_components</code> parameter value is set to 2 and
<code>perplexity</code> parameter value to 30. Also check the
<code>perplexity</code> parameter value of 1 and note your
observations.</li>
</ol>
<p>For more information, refer to the following resources:</p>
<ul>
<li>Sections <strong>6.3: Principal Component Analysis</strong> and
<strong>6.13: t-Distributed Stochastic Neighbor Embedding</strong> of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem AlpaydÄ±n</li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</a></li>
<li><a href="https://distill.pub/2016/misread-tsne/"
class="uri">https://distill.pub/2016/misread-tsne/</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="1" id="b6kwFG8gpStX">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Write your code here</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Load and standardize the Wine dataset</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>wine <span class="op">=</span> load_wine()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> wine.data</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> wine.target</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Standardising the data</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform PCA</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span><span class="fl">0.95</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X_scaled)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate explained variance ratio</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>explained_variance <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Explained variance percentage:&quot;</span>, <span class="bu">sum</span>(explained_variance)<span class="op">*</span><span class="dv">100</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Number of components to achieve 90-95</span><span class="sc">% o</span><span class="st">f explained variance:&quot;</span>, n_components)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform t-SNE</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>X_tsne <span class="op">=</span> tsne.fit_transform(X_scaled)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizations</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA scatter plot</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Principal Component Analysis (PCA)&quot;</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;PC1&quot;</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># t-SNE scatter plot</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_tsne[:, <span class="dv">0</span>], X_tsne[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;t-Distributed Stochastic Neighbor Embedding (t-SNE)&quot;</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;t-SNE 1&quot;</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;t-SNE 2&quot;</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Explained variance percentage: 96.16971684450642

Number of components to achieve 90-95% of explained variance: 0.95
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/aee61959c0c06608a3d88d2613eed142a1713ec4.png" /></p>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="0EHi8mL9-GQK">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>Having experimented with t-SNE using <code>perplexity</code>
parameter values of 1 and 30, what distinctions did you observe in the
visualizations, and which perplexity value do you believe effectively
identifies clusters in the data?</p>
</section>
<div class="cell markdown" id="ILrjkQ6O-lVs">
<p>Perplexity 1: Results in densely packed clusters with a focus on
local relationships. It tends to overlook global structure, potentially
leading to distorted representations of the data. Perplexity 30:
Produces more spread-out clusters, capturing both local and global
structures effectively. This can result in a clearer representation of
the overall data structure. For identifying clusters in the data, a
perplexity value around 30 tends to be more effective as it balances the
consideration of local and global relationships, providing a clearer
visualization of the underlying patterns.</p>
</div>
<section id="question-8-implementing-k-means-clustering"
class="cell markdown" id="lWr2i0CV_MdK">
<h2>Question 8: Implementing k-Means Clustering.</h2>
<p>You have a dataset named 'Mall_Customers.csv,' containing information
about customers' annual income and spending score. Your goal is to
perform customer segmentation using K-means clustering to identify
distinct groups based on their spending behavior and income levels.</p>
<ol>
<li>Select the relevant features for clustering, in this case, 'Annual
Income' and 'Spending Score'.</li>
<li>Standardize the selected features to ensure equal importance during
clustering.</li>
<li>Implement K-means clustering with a specified number of clusters.
Experiment with different number of clusters and set
<code>n_clusters</code> parameter of KMeans to a number which is clearly
differentiates clusters.</li>
<li>Assign the cluster labels back to the original dataset.</li>
<li>Visualize the results by creating a scatter plot, where the x-axis
represents 'Annual Income,' the y-axis represents 'Spending Score,' and
the points are color-coded based on the assigned clusters. <strong>Make
sure to include, xlabel, ylabel and title to the output
plot.</strong></li>
</ol>
<p>For more information, refer to the following resources:</p>
<ul>
<li>Section <strong>7.3: k-Means Clustering</strong> of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem AlpaydÄ±n</li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a></li>
<li><a href="https://www.w3schools.com/python/matplotlib_scatter.asp"
class="uri">https://www.w3schools.com/python/matplotlib_scatter.asp</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="8" id="C17XhMWE_LqL">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Load your dataset (Assuming you have a CSV file called &#39;customer_data.csv&#39;)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment%204/Mall_Customers.csv&#39;</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">#Write your code here</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Select only &#39;Annual Income&#39; and &#39;Spending Score&#39;</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">&#39;Annual Income&#39;</span>, <span class="st">&#39;Spending Score&#39;</span>]]</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the features</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>X_scaled <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform K-means clustering</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)  <span class="co"># Assuming 5 clusters</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X_scaled)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign cluster labels back to the dataset</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&#39;Cluster&#39;</span>] <span class="op">=</span> kmeans.labels_</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the results</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[<span class="st">&#39;Annual Income&#39;</span>], data[<span class="st">&#39;Spending Score&#39;</span>], c<span class="op">=</span>data[<span class="st">&#39;Cluster&#39;</span>], cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Annual Income&#39;</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Spending Score&#39;</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;K-means Clustering&#39;</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/63b09a07b77f6be721e8fa17e5699e655a7ca410.png" /></p>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="pzjdhx3eLE7m">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>What choice of the <code>n_clusters</code> parameter yields effective
clustering results, and what insights were gained from the
experimentation?</p>
</section>
<div class="cell markdown" id="AqYKyZMBLlJ3">
<p>The choice of the n_clusters parameter in K-means clustering is
critical for effective clustering results. Experimenting with various
values and using metrics like the elbow method, silhouette score, or gap
statistic helps identify the optimal number of clusters. This
experimentation provides insights into the inherent structure of the
data, revealing meaningful clusters for further analysis or
decision-making.</p>
</div>
<section id="question-9-comparing-clustering-types"
class="cell markdown" id="smgT4hc5nEiV">
<h2>Question 9: Comparing clustering types</h2>
<p>Your task to write a python code that will compare Single-Link
Clustering, Complete-Link Clustering and k-Means Clustering. Please
follow the below instructions to complete the exercise.</p>
<ol>
<li>Load MNIST dataset using <code>fetch_opeml</code> from
<code>sklearn.datasets</code> and normalise X by dividing it by
255.</li>
<li>Perform dimensionality reduction using PCA with
<code>n_components</code> parameter set to 2. (This helps in visualising
our final plots)</li>
<li>For computational reasons, we will be using only a subset of the
data. Select the first 1000 observations of the data and work on those
going forward.</li>
<li>Implement Single-Link Clustering using
<code>AgglomerativeClustering()</code> with <code>n_clusters</code> and
<code>linkage</code> parameters set to 10 and 'single'
respectively.</li>
<li>Implement Complete-Link Clustering using
<code>AgglomerativeClustering()</code> with <code>n_clusters</code> and
<code>linkage</code> parameters set to 10 and 'complete'
respectively.</li>
<li>Implement k-Means Clustering with <code>n_clusters</code> and
<code>random_state</code> parameters set to 10 and 99 respectively.</li>
<li>Plot Single-Link, Complete-Link and k-Means Clustering using
<code>plot_clusters(X, labels, title)</code> function provided with
appropriate titles.</li>
</ol>
<p>For more information, refer to the following resources:</p>
<ul>
<li>Section <strong>7.8: Hierarchical Clustering</strong> of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem AlpaydÄ±n</li>
<li><a
href="https://www.saigeetha.in/post/hierarchical-clustering-types-of-linkages"
class="uri">https://www.saigeetha.in/post/hierarchical-clustering-types-of-linkages</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering"
class="uri">https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="9" id="kJCZuZ7h7yBU">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering, KMeans</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_openml</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to visualize clustering results</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_clusters(X, labels, title):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    unique_labels <span class="op">=</span> np.unique(labels)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> unique_labels:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        cluster_points <span class="op">=</span> X[labels <span class="op">==</span> label]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f&#39;Cluster </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">&#39;</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Write your code here</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Load MNIST dataset</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> fetch_openml(<span class="st">&#39;mnist_784&#39;</span>, version<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> mnist.data</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mnist.target.astype(<span class="bu">int</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform PCA</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose a subset of data</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>subset_size <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>X_subset <span class="op">=</span> X_pca[:subset_size]</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>y_subset <span class="op">=</span> y[:subset_size]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform single-link clustering</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>single_link <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">10</span>, linkage<span class="op">=</span><span class="st">&#39;single&#39;</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>single_link.fit(X_subset)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform complete-link clustering</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>complete_link <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">10</span>, linkage<span class="op">=</span><span class="st">&#39;complete&#39;</span>)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>complete_link.fit(X_subset)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform KMeans clustering for comparison</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X_subset)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="co">############################################################</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the results</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>plot_clusters(X_subset, y_subset, title<span class="op">=</span><span class="st">&#39;Clustering with Actual Labels&#39;</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>plot_clusters(X_subset, single_link.labels_, title<span class="op">=</span><span class="st">&#39;Single-Link Clustering&#39;</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>plot_clusters(X_subset, complete_link.labels_, title<span class="op">=</span><span class="st">&#39;Complete-Link Clustering&#39;</span>)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>plot_clusters(X_subset, kmeans.labels_, title<span class="op">=</span><span class="st">&#39;KMeans Clustering&#39;</span>)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/21fd285eb1c2bb045723f3114dea4e9580dff58b.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/2465c58116d66fbc631927b6d9d40a07268b5c44.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/42bd512376077961e01b324b81cd5b7251615730.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_2b514194b5f84b4bb12fe8f5108cfe56/29e8338aa04d371c65611d59ad0c2f1220dfb701.png" /></p>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="gn2-tEN2DtPS">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>Which clustering method exhibits the least satisfactory performance,
and provide any insights gained from examining all three clustering
plots and comparing it with the actual label plot.</p>
</section>
<div class="cell markdown" id="MGd12ckIEFQb">
<p>Single-link hierarchical clustering doesn't do as well as the other
methods. When we look at its plot compared to the real labels, the
clusters it makes are more stretched out and not as clear. It seems to
have trouble separating the groups neatly. On the other hand, the other
two methods, complete-link hierarchical clustering and KMeans
clustering, make clusters that look more like the real groups in the
data. This shows that picking the right method is important for getting
good results.</p>
</div>
</body>
</html>
