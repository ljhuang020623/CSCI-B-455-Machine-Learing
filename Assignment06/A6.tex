\documentclass{article}
\usepackage{amsmath}

\begin{document}

\section*{Assignment 06: Theoretical Questions}

\subsection*{Question 1: Gradient Descent Calculation}
Given the function \( E(w) = w^2 - 4w + 4 \), perform gradient descent to find the minimum value of \( w \) over three iterations:
\begin{enumerate}
    \item First, calculate the derivative of \( E(w) \):
    \[
    E'(w) = 2w - 4
    \]

    \item Starting values:
    \begin{itemize}
        \item Initial \( w_0 = 0 \)
        \item Learning rate \( \eta = 0.1 \)
    \end{itemize}

    \item Iteration calculations:
    \begin{itemize}
        \item Iteration 1:
        \[
        w_1 = w_0 - \eta \cdot E'(w_0) = 0 - 0.1 \cdot (2 \cdot 0 - 4) = 0.4
        \]
        \item Iteration 2:
        \[
        w_2 = w_1 - \eta \cdot E'(w_1) = 0.4 - 0.1 \cdot (2 \cdot 0.4 - 4) = 0.72
        \]
        \item Iteration 3:
        \[
        w_3 = w_2 - \eta \cdot E'(w_2) = 0.72 - 0.1 \cdot (2 \cdot 0.72 - 4) = 0.976
        \]
    \end{itemize}
\end{enumerate}
After three iterations, the value of \( w \) is approximately \( 0.976 \).

\subsection*{Question 2: Linear Discriminant Function}
Given \( w_1 = 2, w_2 = -3, w_0 = 5 \):
\begin{enumerate}
    \item For point \( (1, 2) \):
    \[
    g(1, 2) = 2 \cdot 1 - 3 \cdot 2 + 5 = 1
    \]
    \item Classification: Since \( g(1, 2) > 0 \), the point belongs to class C1 (Cancer Positive).

    \item For point \( (3, 4) \):
    \[
    g(3, 4) = 2 \cdot 3 - 3 \cdot 4 + 5 = -1
    \]
    \item Classification: Since \( g(3, 4) < 0 \), the point belongs to class C2 (Cancer Negative).
\end{enumerate}

\subsection*{Question 3: Gradient Descent and Learning Rate}
\begin{enumerate}
    \item[a)] \textbf{Gradient descent} is an optimization algorithm used to minimize a function by iteratively moving towards the minimum value of the function. It is used in machine learning to find the optimal parameters of a model that minimize the cost function.
    \item[b)] \textbf{Learning rate} determines the size of the steps taken in the gradient descent algorithm. A higher learning rate can cause overshooting of the minimum, while a too-small learning rate can result in a long convergence time. It balances the speed and accuracy of convergence.
\end{enumerate}

\subsection*{Question 4: Linear Discriminant and Ranking}
\begin{enumerate}
    \item[a)] \textbf{Linear discriminant models} attribute importance to input features based on the coefficients associated with each feature. Larger absolute values indicate higher importance in class separation.
    \item[b)] \textbf{Ranking} in machine learning involves ordering or prioritizing items or instances, unlike classification which categorizes items into predefined classes and regression which predicts a continuous value. Ranking is particularly useful in recommendation systems and search engines.
\end{enumerate}

\subsection*{Question 5: Decision Trees and Pruning}
\begin{enumerate}
    \item[a)] \textbf{Pruning in decision trees} is the process of removing parts of the tree to prevent overfitting. Pruning improves the model's generalization capabilities by reducing its complexity. Methods include cost-complexity pruning and reduced error pruning. Pruning might not be advisable when the dataset is small or the decision tree is already performing well with high generalization.
    \item[b)] \textbf{Regression with decision trees} can be performed by using the same decision tree algorithm to predict continuous values instead of categories. The tree splits the data into nodes with the target value being the mean or median of the values in each leaf node.
    \item[c)] \textbf{Rule induction from decision trees} converts the paths from the root to the leaf into a set of if-then rules. This approach enhances model interpretability, allowing users to understand the decision-making process more clearly.
\end{enumerate}

\subsection*{Question 6: Entropy and Example Calculation}
\begin{enumerate}
    \item[a)] \textbf{Entropy} is a measure of randomness or uncertainty in information theory. In decision trees, it quantifies the impurity or unpredictability present in the dataset. It is a crucial metric used to determine the best feature that can split the dataset in each node of the tree.
    
    \item[b)]
    Given the counts for the target variable \textit{Play Tennis}, we have:
    \begin{itemize}
        \item Number of \textit{Yes} instances: $9$
        \item Number of \textit{No} instances: $5$
        \item Total number of instances: $9 + 5 = 14$
    \end{itemize}
    
    The proportions are:
    \begin{align*}
        p_{\text{Yes}} &= \frac{\text{Number of Yes instances}}{\text{Total number of instances}} = \frac{9}{14} \\
        p_{\text{No}} &= \frac{\text{Number of No instances}}{\text{Total number of instances}} = \frac{5}{14}
    \end{align*}
    
    The entropy \( H(S) \) for the target variable \textit{Play Tennis} is calculated using the formula:
    
    \[
    H(S) = -p_{\text{Yes}} \log_2(p_{\text{Yes}}) - p_{\text{No}} \log_2(p_{\text{No}})
    \]
    
    Plugging in the proportions, we get:
    
    \[
    H(S) = -\left(\frac{9}{14}\right) \log_2\left(\frac{9}{14}\right) - \left(\frac{5}{14}\right) \log_2\left(\frac{5}{14}\right)
    \]
    
    Computing the values:
    
    \begin{align*}
        H(S) &\approx -\left(\frac{9}{14}\right) \log_2\left(\frac{9}{14}\right) - \left(\frac{5}{14}\right) \log_2\left(\frac{5}{14}\right) \\
        &\approx -\left(\frac{9}{14}\right) \times (-0.1520) - \left(\frac{5}{14}\right) \times (-0.4307) \\
        &\approx 0.9403
    \end{align*}
    
    Therefore, the entropy of the dataset for the target variable \textit{Play Tennis} is approximately $0.9403$.
\end{enumerate}

\end{document}
