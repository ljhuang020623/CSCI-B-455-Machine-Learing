<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>7b53d79210974dfe812d6546c627dfe4</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="important-instruction" class="cell markdown"
id="E2WR-yMIpcbm">
<h2>Important instruction</h2>
<p>For programming exercises that only editing, only edit the code as
shown in the following format.</p>
<pre><code>##############################################

#Edit the following code

var1 = 3
var2 = 4
print(var1 + var4)

##############################################</code></pre>
<p>You are open to experimenting with the other parts of code but you
will only be awarded points if the question asked is answered which only
needs finishing or making changes to the code in the above specified
format.</p>
</section>
<section id="question-7-visualising-classification-decision-trees"
class="cell markdown" id="pUm8wagILU0-">
<h2>Question 7: Visualising Classification Decision Trees</h2>
<p>The objective of this question is to implement a Binary
Classification Decision Tree with visualizations. Follow the given
instructions below:</p>
<h4 id="step-1-import-required-libraries">Step 1: Import Required
Libraries</h4>
<h4 id="step-2-load-and-preprocess-the-data">Step 2: Load and Preprocess
the Data</h4>
<ul>
<li>Load the Iris dataset using <code>load_iris()</code>.</li>
<li>Convert the target variable to represent a binary classification: 1
for 'Iris-setosa' and 0 for others.</li>
</ul>
<h4 id="step-3-split-the-dataset">Step 3: Split the Dataset</h4>
<ul>
<li>Divide the dataset into training and testing sets using
<code>train_test_split()</code>, with an 80-20 split and a random state
of 99.</li>
</ul>
<h4 id="step-4-initialize-and-train-the-classifier">Step 4: Initialize
and Train the Classifier</h4>
<ul>
<li>Initialize the <code>DecisionTreeClassifier</code> with a random
state of 99.</li>
<li>Train the classifier on the training data.</li>
</ul>
<h4 id="step-5-visualize-the-decision-tree">Step 5: Visualize the
Decision Tree</h4>
<ul>
<li>Create a basic visualization of the decision tree. As shown
below.</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q9-1.png"
alt="plot1" /></p>
<!-- - Create a detailed visualization of the decision tree on the second subplot as shown below with all the lables and styles.

![plot2](https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q9-2.png) -->
<p>For more information, refer to the following resources:</p>
<ul>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a></li>
<li><a
href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html"
class="uri">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="4" id="uT2Y1fAYAq1e">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_iris</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> load_iris()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris.data</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.where(iris.target <span class="op">==</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)  <span class="co"># 1 for &#39;Iris-setosa&#39; and 0 for other</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.2</span>, random_state <span class="op">=</span> <span class="dv">99</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> DecisionTreeClassifier(random_state<span class="op">=</span> <span class="dv">99</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">20</span>,<span class="dv">10</span>))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plot_tree(clf,filled <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_28b7f1b9fea7474fb36cc9df7e528ff4/6a615051fb2af9b9f42530d7719640527268d6b9.png" /></p>
</div>
</div>
<section id="question-8-regression-decision-tree" class="cell markdown"
id="OnZllgi5Ldel">
<h2>Question 8: Regression Decision Tree</h2>
<p>The objective of this question is to implement a Binary
Classification Decision Tree with visualizations. Follow the given
instructions below:</p>
<h4 id="step-1-import-required-libraries">Step 1: Import Required
Libraries</h4>
<h4 id="step-2-load-and-preprocess-the-data">Step 2: Load and Preprocess
the Data</h4>
<ul>
<li>Use this data: <a
href="https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/data-rdt.csv"
class="uri">https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/data-rdt.csv</a></li>
</ul>
<h4 id="step-3-split-the-data">Step 3: Split the Data:</h4>
<ul>
<li>The data is split into training and testing sets, with 20% reserved
for testing. The random state is set to 99.</li>
</ul>
<h4 id="step-4-train-the-decision-tree-regressor">Step 4: Train the
Decision Tree Regressor:</h4>
<ul>
<li>A <code>DecisionTreeRegressor</code> is instantiated with a maximum
depth and then trained on the training data. Choose a max_depth value
that will generate the same plot as below. The range of max_depth values
that you could experiment is from 1 to 5.</li>
</ul>
<p><img
src="https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/imagedata/a6q10.png"
alt="plot3" /></p>
<h4 id="step-5-evaluate-the-model">Step 5: Evaluate the Model:</h4>
<ul>
<li>The model makes predictions on the test set, and the Mean Squared
Error (MSE) is calculated to evaluate the model's accuracy. The MSE is
printed to the console.</li>
</ul>
<h4 id="step-6-visualize-the-results">Step 6: Visualize the
Results:</h4>
<ul>
<li>Replicate the plot that was shown above.</li>
</ul>
<p>For more information, refer to the following resources:</p>
<ul>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</a></li>
<li><a
href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html"
class="uri">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="2" id="w2tYICCffyrb">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>data_url <span class="op">=</span> <span class="st">&quot;https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/data-rdt.csv&quot;</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(data_url)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data.iloc[:, :<span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data.iloc[:, <span class="op">-</span><span class="dv">1</span>].values</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">99</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>regressor <span class="op">=</span> DecisionTreeRegressor(max_depth <span class="op">=</span> <span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">99</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>regressor.fit(X_train, y_train)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> regressor.predict(X_test)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Mean Squared Error: </span><span class="sc">{</span>mse<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>indices_sorted <span class="op">=</span> X_test.flatten().argsort()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>X_test_sorted <span class="op">=</span> X_test[indices_sorted]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>y_pred_sorted <span class="op">=</span> y_pred[indices_sorted]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test, y_test, color<span class="op">=</span><span class="st">&#39;blue&#39;</span>, label<span class="op">=</span><span class="st">&#39;Data&#39;</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test_sorted, y_pred_sorted, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Prediction&#39;</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Regression Tree with chosen max_depth&#39;</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Mean Squared Error: 0.015499944200849373
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_28b7f1b9fea7474fb36cc9df7e528ff4/23ed9a8a200af241e5297953cc6165f82e87c2b5.png" /></p>
</div>
</div>
<section
id="question-9-comparing-linear-discriminant-and-logistic-discriminant"
class="cell markdown" id="g5OcsDcqLgOH">
<h2>Question 9: Comparing Linear Discriminant and Logistic
Discriminant</h2>
<p>The objective of this question is to implement and compare Linear
Discriminant and Logistic Discriminant. Follow the given instructions
below:</p>
<h3 id="step-1-load-the-dataset">Step 1: Load the Dataset</h3>
<ul>
<li>Use this data: <a
href="https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/lin_log.csv"
class="uri">https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/lin_log.csv</a></li>
</ul>
<h3 id="step-2-split-the-data">Step 2: Split the Data</h3>
<ul>
<li>Divide the dataset into two parts: one for training the models and
the other for testing their performance. A common split ratio is 70% for
training and 30% for testing. Use 99 as the random state value.</li>
</ul>
<h3 id="step-3-preprocess-the-data">Step 3: Preprocess the Data</h3>
<ul>
<li>Perform any necessary data preprocessing steps, such as feature
scaling. Standardizing the features so they have a mean of 0 and a
standard deviation of 1 can help improve the performance of many machine
learning algorithms.</li>
</ul>
<h3 id="step-4-train-logistic-regression-with-polynomial-features">Step
4: Train Logistic Regression with Polynomial Features</h3>
<ul>
<li>Enhance the Logistic Regression model by adding polynomial features,
which allow the model to capture more complex relationships in the data.
Use a degree of polynomial that you find suitable based on the
complexity of your dataset.</li>
<li>Train the enhanced Logistic Regression model on the training
dataset.</li>
</ul>
<h3 id="step-5-train-linear-discriminant-analysis-lda">Step 5: Train
Linear Discriminant Analysis (LDA)</h3>
<ul>
<li>Train an LDA model on the training dataset. LDA tries to find a
linear combination of features that best separates two or more classes
of events.</li>
</ul>
<h3 id="step-6-evaluate-the-models">Step 6: Evaluate the Models</h3>
<ul>
<li>Use the testing dataset to evaluate the performance of both trained
models. Calculate metrics such as accuracy to understand how well each
model performs.</li>
</ul>
<h3 id="step-7-visualize-decision-boundaries">Step 7: Visualize Decision
Boundaries</h3>
<ul>
<li>Plot the decision boundaries for each model to visualize how they
separate the classes in the dataset. Use the
<code>plot_decision_boundaries</code> function for plotting the decision
boundaries.</li>
</ul>
<h3 id="step-8-compare-and-interpret-results">Step 8: Compare and
Interpret Results</h3>
<ul>
<li>Compare the performance of the two models based on the accuracy
metrics and the visualized decision boundaries.</li>
</ul>
<p>For more information, refer to the following resources:</p>
<ul>
<li>Chapter <strong>10. Linear Discrimination</strong> of
<strong>Introduction to Machine Learning, 4th Edition, The MIT Press,
2020</strong> by Ethem Alpaydin.</li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html</a></li>
<li><a
href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html"
class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html</a></li>
<li><a
href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html"
class="uri">https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html</a></li>
<li><a
href="https://www.introspective-mode.org/logistic-regression-or-discriminant-function-analysis/"
class="uri">https://www.introspective-mode.org/logistic-regression-or-discriminant-function-analysis/</a></li>
</ul>
</section>
<div class="cell code" data-execution_count="10" id="AIx9XV9zwYTz">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, PolynomialFeatures</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.discriminant_analysis <span class="im">import</span> LinearDiscriminantAnalysis</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to plot decision boundaries</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundaries(X, y, model, title):</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">.02</span>  <span class="co"># step size in the mesh</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h),</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>                         np.arange(y_min, y_max, h))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.5</span>, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">20</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">##############################################</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co">#Write your code from here</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/KrishnaTejaJ/datasets-CSCI-B455/main/assignment6/lin_log.csv&#39;</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">&#39;Feature_1&#39;</span>, <span class="st">&#39;Feature_2&#39;</span>]].values</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">&#39;Target&#39;</span>].values</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>X_train,X_test, y_train,y_test <span class="op">=</span> train_test_split(X, y, test_size <span class="op">=</span> <span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">99</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>scalar <span class="op">=</span> StandardScaler()</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>Xscaled <span class="op">=</span> scalar.fit_transform(X_train)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>xtest_scaled <span class="op">=</span> scalar.transform(X_test)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>log_reg <span class="op">=</span> LogisticRegression(random_state<span class="op">=</span> <span class="dv">99</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>poly <span class="op">=</span> PolynomialFeatures(degree <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>logReg <span class="op">=</span> make_pipeline(poly,log_reg)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>logReg.fit(Xscaled, y_train)</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>lda <span class="op">=</span> LinearDiscriminantAnalysis()</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>lda.fit(Xscaled , y_train)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>y_pred_logReg <span class="op">=</span> logReg.predict(xtest_scaled)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>y_pred_lda <span class="op">=</span> lda.predict(xtest_scaled)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Accuracy of Logistic Regression: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_logReg)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Accuracy of LDA: </span><span class="sc">{</span>accuracy_score(y_test, y_pred_lda)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>plot_decision_boundaries(Xscaled, y_train, logReg, <span class="st">&#39;Logistic Regression&#39;</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co">#plot_decision_boundaries(Xscaled, y_train, lda, &#39;Linear Discriminant Analysis&#39;)</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy of Logistic Regression: 0.89
Accuracy of LDA: 0.8866666666666667
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_28b7f1b9fea7474fb36cc9df7e528ff4/c3687e15a8228e204595bbf98afba28a9718cf3a.png" /></p>
</div>
</div>
<section id="answer-the-following-question-with-a-brief-reasoning"
class="cell markdown" id="pzjdhx3eLE7m">
<h3><strong>Answer the following question with a brief
reasoning.</strong></h3>
<p>After comparing the Linear Discriminant and Logistic Discriminant,
which method do you think draws the most appropriate decision boundary
and <strong>why</strong>?</p>
</section>
<div class="cell markdown" id="AqYKyZMBLlJ3">
<p>Logistic Regression offers flexibility with non-linear boundaries,
potentially fitting complex patterns better, while LDA assumes linear
separability and equal class covariances, providing a simpler model.
Given the similar accuracy levels but the added flexibility of capturing
non-linear patterns, I would lean towards the polynomial-featured
Logistic Regression for this particular dataset. It likely provides a
more nuanced decision boundary that could potentially generalize better
to unseen data, assuming it does not overfit.</p>
</div>
</body>
</html>
