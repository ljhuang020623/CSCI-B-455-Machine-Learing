\documentclass{article}
\usepackage{amsmath}
\title{HW1}
\author{LJ Huang}
\date{February 11}

\begin{document}
\maketitle
\section*{Question1:}
\indent\indent Event A: Drawing a red ball.

Event B: Drawing a ball that is either red or green.

Event C: Drawing a ball that is not blue.

\subsection*{a.}

The conditional probability of Event \(A\) given Event \(B\) can be calculated 
\indent without using Bayesâ€™ theorem as follows:

\[P(A|B) = \frac{\text{Number of favorable outcomes for } A \text{ within } B}{\text{Total number of outcomes in } B} = \frac{8}{13}\]

This is because the number of favorable outcomes for \(A\) within \(B\) is the 
\indent number of red balls (8), and the total number of outcomes in \(B\) (drawing a 
\indent red or green ball) is the sum of red and green balls, which is \(8 + 5 = 13\).

Therefore, 

\[P(A|B) = P(A|C) = \frac{8}{13}.\]

\subsection*{b.}

Let the total number of balls be $20$ ($8$ red, $5$ green, and $7$ blue).

The unconditional probability of drawing a red ball (Event A) is:
\[P(A) = \frac{\text{Number of red balls}}{\text{Total number of balls}} = \frac{8}{20} = \frac{2}{5}\]

Given Event C is drawing a ball that is not blue (i.e., drawing a red or 
\indent green ball), the conditional probability of drawing a red ball (Event A) given 
\indent Event C is:
\[P(A|C) = \frac{8}{13}\]

To check if Event A is conditionally dependent on Event C, we compare 
\indent $P(A)$ with $P(A|C)$. Since $P(A) = \frac{2}{5}$ and $P(A|C) = \frac{8}{13}$, and these probab
\indent ilities are not equal, Event A is conditionally dependent on Event C.

\subsection*{c.}

\indent Bayes' Theorem provides a way to update our probabilities based on new 
\indent evidence. The theorem is stated as follows:

\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]

{\small\emph Proof.}

\begin{enumerate}
    \item Begin with the definition of conditional probability:
    \[P(A|B) = \frac{P(A \cap B)}{P(B)}\]
    
    \item The joint probability of A and B can also be expressed using conditional probability:
    \[P(A \cap B) = P(B|A)P(A)\]
    
    \item Substituting the expression for \(P(A \cap B)\) from step 2 into the formula from step 1, we get:
    \[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]
    
    \item This equation represents Bayes' Theorem, showing how the probability of A given B is related to the probability of B given A, the probability of A, and the probability of B.
\end{enumerate}

\section*{Question2:}
\subsection*{a.}

In a Bayesian statistical framework, considering the event of tossing a coin, where the probability of getting a head is 0.5 with a prior belief of 0.9 and 0.6 with a prior belief of 0.1, we find the posterior distributions given that we observe 5 consecutive heads.

The posterior distribution is given by the formula:
\[ \text{Posterior} \propto \text{Likelihood} \times \text{Prior} \]

For \(p=0.5\), the likelihood of observing 5 consecutive heads is \(0.5^5\). The prior belief is 0.9. Thus, the unnormalized posterior is proportional to \(0.5^5 \times 0.9\).

For \(p=0.6\), the likelihood of observing 5 consecutive heads is \(0.6^5\). The prior belief is 0.1. Thus, the unnormalized posterior is proportional to \(0.6^5 \times 0.1\).

\subsection*{b.}

Given a coin with an unknown probability of landing heads \(p\), and initially believing (prior) that \(p\) follows a \(Beta(2, 2)\) distribution, we find the posterior distribution after observing 10 consecutive tails.

Given that the likelihood function for observing 10 consecutive tails is \((1-p)^{10}\), and the prior distribution is \(Beta(2, 2)\), the posterior distribution after observing the data is updated as follows:
\[ \text{Posterior} = Beta(2+0, 2+10) = Beta(2, 12) \]

This is because the Beta distribution's parameters are updated by adding the number of observed successes to \(a\) and the number of observed failures to \(b\), giving us a new posterior distribution of \(Beta(2, 12)\).

\section*{Question3:}
\subsection*{1.}
Given the Gaussian density function:
\[ f(x_i|\Theta) = \frac{1}{\sqrt{2\pi\Theta_1}} \exp\left(-\frac{(x_i - \Theta_0)^2}{2\Theta_1}\right) \]

The likelihood function is:
\[ L(\Theta) = \prod_{i=1}^{n} f(x_i|\Theta) = \left(\frac{1}{\sqrt{2\pi\Theta_1}}\right)^n \exp\left(-\sum_{i=1}^{n}\frac{(x_i - \Theta_0)^2}{2\Theta_1}\right) \]

The log-likelihood function becomes:
\[ \ell(\Theta) = -\frac{n}{2}\log(2\pi\Theta_1) - \frac{1}{2\Theta_1}\sum_{i=1}^{n}(x_i - \Theta_0)^2 \]
\subsection*{2.}
Taking derivatives and setting them to zero to solve for $\Theta_0$ and $\Theta_1$

\section*{Question4:}
\subsection*{1.}
Given the Bernoulli distribution function:
\[ P(x) = p^x \cdot (1 - p)^{1-x}, \quad x \in \{0, 1\} \]

The likelihood function for a sample \(x_1, x_2, ..., x_n\) is:
\[ L(p|x) = \prod_{i=1}^{n} p^{x_i} \cdot (1 - p)^{1-x_i} \]

Maximizing the log-likelihood function:
\[ \ell(p) = \log L(p|x) = \sum_{i=1}^{n} \left[ x_i \log(p) + (1 - x_i) \log(1 - p) \right] \]

Taking the derivative and setting it to zero gives:
\[ p = \frac{\sum_{i=1}^{n} x_i}{n} \]

\subsection*{2.}

For the event of rolling 2 dice, with \(x = 1\) if the sum is 9 and \(x = 0\) otherwise, and with 4 outcomes where the sum is 9 out of 36 total outcomes, \(p\) is calculated as:
\[ p = \frac{4}{36} = \frac{1}{9} \]

Thus, based on the rolling of 2 dices, the value of \(p\) is \(\frac{1}{9}\).

\section*{Question5:}
Given the scenario where the rate of coming to rest on a swing, denoted by $\Theta$, can either be 2 or 4, with prior probabilities $P(\Theta = 2) = 0.7$ and $P(\Theta = 4) = 0.3$, we aim to calculate the posterior probabilities of $\Theta$ given the observation of 5 children playing.

The likelihood of observing 5 children given $\Theta$ is provided as:
\[ P(X = 5 | \Theta = 2) = 0.022 \]
\[ P(X = 5 | \Theta = 4) = 0.105 \]

Using Bayes' theorem, the posterior probability is calculated as:
\[ P(\Theta | X) = \frac{P(X | \Theta) \cdot P(\Theta)}{P(X)} \]

Where $P(X)$, the total probability of observing 5 children, can be found using the law of total probability:
\[ P(X = 5) = P(X = 5 | \Theta = 2) \cdot P(\Theta = 2) + P(X = 5 | \Theta = 4) \cdot P(\Theta = 4) \]
\[ P(X = 5) = 0.022 \cdot 0.7 + 0.105 \cdot 0.3 = 0.0469 \]

Thus, the posterior probabilities are:
\[ P(\Theta = 2 | X = 5) = \frac{0.022 \cdot 0.7}{0.0469} \approx 0.328 \]
\[ P(\Theta = 4 | X = 5) = \frac{0.105 \cdot 0.3}{0.0469} \approx 0.672 \]

Therefore, after observing 5 children playing, the probability of $\Theta = 2$ is approximately 0.328, and the probability of $\Theta = 4$ is approximately 0.672, indicating a shift in belief towards $\Theta$ being 4 rather than 2 based on the observed evidence.

\section*{Question6:}
\section{Bayes' Estimator}

The Bayes' Estimator in Bayesian statistics is a method for estimating a parameter by utilizing the posterior distribution, which combines prior beliefs about the parameter with the likelihood of the observed data. This estimator updates our knowledge by calculating either the mode or the expectation of the posterior distribution, reflecting a balance between prior information and new evidence.

\section{Influence of Prior Information}

In essence, prior information in Bayesian estimation allows for a nuanced approach to statistical analysis, merging past knowledge and new data to form updated beliefs.



\end{document}